{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from utils import nltk_tag_to_wordnet_tag, lemmatize_sentence\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import os \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "l = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained word2vector weight loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "'''\n",
    "import the skip-gram model \n",
    "'''\n",
    "EMB_DIM = 100\n",
    "w2v = Word2Vec.load(\"word2vec_iter10_DIM100.model\")\n",
    "word_vectors = w2v.wv\n",
    "embedding_matrix = word_vectors.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Element list',\n",
       " 'Defect',\n",
       " 'Defect_Brick masonry',\n",
       " 'Defect_Stone',\n",
       " 'Defect_Concrete',\n",
       " 'Defect_Metal and glass',\n",
       " 'Defect_Glass',\n",
       " 'Defect_Metal',\n",
       " 'Attributes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the sheets of lists\n",
    "\n",
    "xls_name = 'Labeling list_v4.xlsx'\n",
    "sheet_names = pd.ExcelFile(xls_name).sheet_names\n",
    "\n",
    "sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Material Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIt will load defect words from the excel spreadsheet and also find matched reports in the next cell.\\n\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############## HERE #################\n",
    "defect_sheet = 'Defect_Concrete'\n",
    "############## HERE #################\n",
    "\n",
    "'''\n",
    "It will load defect words from the excel spreadsheet and also find matched reports in the next cell.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read word list and make a comprehensive list for all tenses and format.\n",
    "'''\n",
    "\n",
    "components = pd.read_excel(xls_name, sheet_name=sheet_names[0], header=None)\n",
    "defects = pd.read_excel(xls_name, sheet_name=defect_sheet, header=None)\n",
    "attributes = pd.read_excel(xls_name, sheet_name='Attributes', header=0)\n",
    "\n",
    "#componnet and defect to lists\n",
    "comp = components[0].tolist()\n",
    "defe = defects[0].tolist()\n",
    "\n",
    "#lower case for components and defects \n",
    "comp = [x.lower() for x in comp]\n",
    "defe = [x.lower() for x in defe]\n",
    "\n",
    "#attribute list\n",
    "attr = np.array(attributes).flatten()\n",
    "attr = pd.DataFrame(attr).dropna()[0].tolist()\n",
    "\n",
    "#eliminate duplicates for comp, defect\n",
    "comp = np.unique(comp)\n",
    "defe = np.unique(defe)\n",
    "\n",
    "#eliminate space in defect list\n",
    "defe = [x[:-1] if x[-1] == ' ' else x for x in defe]\n",
    "\n",
    "from pattern.en import pluralize, singularize\n",
    "\n",
    "#make sure to consider plr anf single nouns\n",
    "comp0 = comp\n",
    "comp1 = [pluralize(x) for x in comp]\n",
    "comp2 = [singularize(x) for x in comp]\n",
    "comp = [[x,y,z] for x,y,z in zip(comp0.tolist(),comp1,comp2)]\n",
    "comp = np.array(comp).flatten().tolist()\n",
    "\n",
    "from pattern.en import conjugate\n",
    "\n",
    "'''\n",
    "process defect list\n",
    "'''\n",
    "\n",
    "#combining all formats and tenses of wordings together \n",
    "for i in range(2):\n",
    "    try:\n",
    "        dat = []\n",
    "        dat.append(defe)\n",
    "        for t in ['inf', '1sg', '3sg', '1sgp', 'pl', 'ppl', 'part', 'ppart']:\n",
    "            dat.append(np.array(list(map(lambda x: conjugate(x, t), defe))))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "dat = np.array(dat).T\n",
    "defe = dat\n",
    "defe = np.array(defe).flatten()\n",
    "defe = defe.tolist()\n",
    "\n",
    "#--> for multi wordings \n",
    "tokenizer = MWETokenizer([x.split(' ') for x in comp] + [x.split(' ') for x in defe] + [x.split(' ') for x in attr])\n",
    "\n",
    "#tokenized component, defect, and attribute\n",
    "comp = [tokenizer.tokenize(x.split()) for x in comp]\n",
    "defe = [tokenizer.tokenize(x.split()) for x in defe]\n",
    "attr = [tokenizer.tokenize(x.split()) for x in attr]\n",
    "\n",
    "comp = [x[0] for x in comp]\n",
    "defe = [x[0] for x in defe]\n",
    "attr = [x[0] for x in attr]\n",
    "\n",
    "### Read report ###\n",
    "\n",
    "fpath = glob('0_Spellcheck_completed/*')\n",
    "\n",
    "if defect_sheet == 'Defect_Concrete':\n",
    "    file_path = [fpath[1]]\n",
    "elif defect_sheet == 'Defect_Brick masonry':\n",
    "    file_path = [fpath[3]]\n",
    "elif defect_sheet == 'Defect_Stone':\n",
    "    file_path = [fpath[6]]\n",
    "elif defect_sheet == 'Defect_Metal and glass':\n",
    "    file_path = [fpath[4]]\n",
    "elif defect_sheet == 'Defect_Glass':\n",
    "    file_path = [fpath[5]]\n",
    "else:\n",
    "    0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0_Spellcheck_completed/corrected_by_MS_API_Concrete.txt']\n"
     ]
    }
   ],
   "source": [
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = [' floor' if '_floor' in x else x for x in attr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read reports and tokenize into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f_ in file_path:\n",
    "    txt = ''\n",
    "    with open(f_, 'r', encoding='utf-8') as f:\n",
    "        txt += ' ' + f.read()\n",
    "\n",
    "#fix typos in text\n",
    "txt = txt.replace('.', '. ')\n",
    "\n",
    "txt = txt.replace('SWARMP', 'SWARMP ')\n",
    "txt = txt.replace('/', ' or  ')\n",
    "txt = txt.replace('.', ' . ')\n",
    "txt = txt.replace(\"'\", \" ' \")\n",
    "txt = txt.replace('\"', ' \" ')\n",
    "txt = txt.replace('`', ' ` ')\n",
    "txt = txt.replace('categorizedas', 'categorized as')\n",
    "txt = txt.replace('rangedbetween', 'ranged between')\n",
    "txt = txt.replace('wasobserved', 'was observed')\n",
    "\n",
    "# sentence tokenization\n",
    "sent_ = sent_tokenize(txt.lower())\n",
    "sent_ = list(map(lambda x: x.replace('n/a', 'n/a '), sent_))\n",
    "sent_ = list(map(lambda x: x.replace('/', ' / '), sent_))\n",
    "sent_ = list(map(word_tokenize,sent_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10040"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total number of sentences: Brick masonry 409097; \n",
    "len(sent_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the tag matrix for sentence list\n",
    "#for each word in the sentence, the initial tag was 'O'\n",
    "\n",
    "\n",
    "\n",
    "sent_tag = []\n",
    "for s in sent_:\n",
    "    tag = []\n",
    "    for w in s:\n",
    "        tag.append('O')\n",
    "    sent_tag.append(tag)\n",
    "\n",
    "#tokenize sentences\n",
    "sent_ = [tokenizer.tokenize(x) for x in sent_]\n",
    "\n",
    "#combine sentence with sentence tag matrix\n",
    "dict_sent = []\n",
    "for s,t in zip(sent_,sent_tag):\n",
    "    dict_sent.append([(x,y) for x,y in zip(s,t)])\n",
    "\n",
    "# Different formats of the same component?\n",
    "#Extract index of components in sentences\n",
    "idx_comp = []\n",
    "items = set(comp)\n",
    "for x in sent_:\n",
    "    found = [i for i,j in enumerate(x) if j in items]\n",
    "    idx_comp.append(found)\n",
    "\n",
    "# extract index of defects in sentences\n",
    "idx_defe = []\n",
    "items = set(defe)\n",
    "for x in sent_:\n",
    "    found = [i for i,j in enumerate(x) if j in items]\n",
    "    idx_defe.append(found)\n",
    "    \n",
    "# extract index of attributes in sentences\n",
    "idx_attr = []\n",
    "items = set(attr)\n",
    "for x in sent_:\n",
    "    found = [i for i,j in enumerate(x) if j in items]\n",
    "    idx_attr.append(found)\n",
    "\n",
    "for st,idx_c in zip(sent_tag,idx_comp):\n",
    "    for idx in idx_c:\n",
    "        st[idx] = 'B-Component'\n",
    "\n",
    "for st,idx_d in zip(sent_tag,idx_defe):\n",
    "    for idx in idx_d:\n",
    "        st[idx] = 'B-Defect'\n",
    "        \n",
    "for st,idx_d in zip(sent_tag,idx_attr):\n",
    "    for idx in idx_d:\n",
    "        st[idx] = 'B-Attr'\n",
    "\n",
    "'''\n",
    "parsing through sentences and tags, combined them together.\n",
    "'''\n",
    "\n",
    "dict_sent = []\n",
    "for s,t in zip(sent_,sent_tag):\n",
    "    dict_sent.append([(x,y) for x,y in zip(s,t)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split B to B and I\n",
    "\n",
    "'''\n",
    "adding I tag to the inside wordings (multi words tokens)\n",
    "'''\n",
    "for cat in ['Component', 'Defect', 'Attr']:\n",
    "    '10: max length of the multiword tokens'\n",
    "    for k in range(10):\n",
    "        for row in dict_sent:\n",
    "            tup = [(i,x) for i,x in enumerate(row) if '_' in x[0] and x[1] == 'B-{}'.format(cat)]\n",
    "            if len(tup) > 0:\n",
    "                tup = tup[0]\n",
    "                x = tup[1]\n",
    "                del row[tup[0]]\n",
    "                a = x[0].split('_')\n",
    "                b = ['B-{}'.format(cat)] + ['I-{}'.format(cat)]*(len(a)-1)\n",
    "                insert_list = [(a_,b_) for a_,b_ in zip(a,b)]\n",
    "\n",
    "                for j in range(len(insert_list)):\n",
    "                    row.insert(j + tup[0], insert_list[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy array \n",
    "dict_sent = [np.array(x) for x in dict_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT preprocessing \n",
    "#FOR analysis: generate input for the table\n",
    "arr = np.vstack(dict_sent)\n",
    "\n",
    "unique, counts = np.unique([x for x in arr if x[1] == 'B-Defect'], return_counts=True)\n",
    "\n",
    "df = pd.DataFrame(np.asarray((unique, counts)).T)\n",
    "df[1] = df[1].astype(int)\n",
    "\n",
    "df.sort_values(by=1,ascending=False)[:30]\n",
    "#Output labeled defects in various forms and the frequency? \n",
    "#Some of the output words are not in the input list. Is this because the labeling is based on lemmatized tokens \n",
    "#and the output are based on original text?\n",
    "\n",
    "#Need to combine the ones that are indicating the same defect type and addup the frequency\n",
    "\n",
    "np.asarray((unique, counts)).T\n",
    "\n",
    "arr = np.hstack([t[:,1] for t in dict_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "O, I, B = 0, 0, 0\n",
    "B_component, B_defect, B_attr = 0, 0, 0\n",
    "\n",
    "for t in arr:\n",
    "    if t == 'O':\n",
    "        O += 1\n",
    "    elif t == 'B-Component':\n",
    "        B_component += 1\n",
    "    elif t == 'B-Defect':\n",
    "        B_defect += 1\n",
    "    elif t == 'B-Attr':\n",
    "        B_attr += 1\n",
    "    elif t[0] == 'I':\n",
    "        I += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3935, 564, 490, 68189)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of components, defects, attributes tokens\n",
    "B_component, B_defect, B_attr, O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBrick masonry: (138206, 22244, 18546, 2570239)\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Brick masonry: (138206, 22244, 18546, 2570239)\n",
    "Stone: (6909, 1346, 1063, 125619)\n",
    "Concrete:(3935, 564, 490, 68189)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select sentences having Component\n",
    "s_component = []\n",
    "for s in dict_sent:\n",
    "    if 'B-Component' in s[:,1]:\n",
    "        s_component.append(s)\n",
    "        \n",
    "s_defect = []\n",
    "for s in dict_sent:\n",
    "    if 'B-Defect' in s[:,1]:\n",
    "        s_defect.append(s)\n",
    "        \n",
    "s_attributes = []\n",
    "for s in dict_sent:\n",
    "    if 'B-Attr' in s[:,1]:\n",
    "        s_attributes.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ = []\n",
    "for s in dict_sent:\n",
    "    if 'B-Component' in s[:,1] or 'B-Defect' in s[:,1] or 'B-Attr' in s[:,1]:\n",
    "        s_.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2689"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of sentences that Component OR defect OR attributes in a sentence\n",
    "len(s_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2341, 446, 421)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_component), len(s_defect), len(s_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brick masonry: 100501 sentences, (84487, 17181, 16134)\n",
    "#Stone: 5403 sentences, (4385, 1023, 959)\n",
    "#concrete:2689 sentences, (2341, 446, 421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need numbers for unique component, defect, and attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the tagged sentences with labels \n",
    "#Ignore the previous ones if the files are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tagged sentences \n",
    "# UNcomment this cell when saving tagging results\n",
    "with open('dict_sent/{}_dict_sent.txt'.format(defect_sheet), 'wb') as f:\n",
    "    pickle.dump(dict_sent,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading the saved tagged files.\n",
    "with open('dict_sent/{}_dict_sent.txt'.format(defect_sheet), 'rb') as f:\n",
    "    dict_sent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split B to B and I\n",
    "\n",
    "for cat in ['Component', 'Defect', 'Attr']:\n",
    "    for k in range(10):\n",
    "        for row in dict_sent:\n",
    "            tup = [(i,x) for i,x in enumerate(row) if '_' in x[0] and x[1] == 'B-{}'.format(cat)]\n",
    "            if len(tup) > 0:\n",
    "                tup = tup[0]\n",
    "                x = tup[1]\n",
    "                del row[tup[0]]\n",
    "                a = x[0].split('_')\n",
    "                b = ['B-{}'.format(cat)] + ['I-{}'.format(cat)]*(len(a)-1)\n",
    "                insert_list = [(a_,b_) for a_,b_ in zip(a,b)]\n",
    "\n",
    "                for j in range(len(insert_list)):\n",
    "                    row.insert(j + tup[0], insert_list[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10040"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_sent)\n",
    "#How many sentences we have for each facade type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concrete: 10040 sentences\n",
    "#brick masonry: 409097\n",
    "#Stone: 18601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_sent = dict_sent[:200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filter sentences that have related information\n",
    "'''\n",
    "s_related = []\n",
    "for s in dict_sent:\n",
    "    if 'B-Component' in s[:,1] or 'B-Defect' in s[:,1] or 'B-Attr' in s[:,1]:\n",
    "        s_related.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2689\n"
     ]
    }
   ],
   "source": [
    "print(len(s_related))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concrete: 2689 sentences\n",
    "#brick masonry: 100501\n",
    "#Stone: 5403"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#considering all format for components and defects?\n",
    "\n",
    "from pattern.en import pluralize, singularize, conjugate\n",
    "\n",
    "def plusing(comp):\n",
    "    comp0 = comp\n",
    "    comp1 = [pluralize(x) for x in comp]\n",
    "    comp2 = [singularize(x) for x in comp]\n",
    "    comp = [[x,y,z] for x,y,z in zip(comp0.tolist(),comp1,comp2)]\n",
    "#     comp = np.array(comp).flatten().tolist()\n",
    "    return comp\n",
    "\n",
    "def conj(defe):\n",
    "    for i in range(2):\n",
    "        try:\n",
    "            dat = []\n",
    "            dat.append(defe)\n",
    "            for t in ['inf', '1sg', '3sg', '1sgp', 'pl', 'ppl', 'part', 'ppart']:\n",
    "                dat.append(np.array(list(map(lambda x: conjugate(x, t), defe))))\n",
    "        except:\n",
    "            pass\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split exclusive sentence set. E.g., set1 include [crack, cracking, cracks] and set2 includes [spall, spalled, spalling]\n",
    "#Need to mainten the 7:3 ratio.\n",
    "#Randomly pick words from training/testing and test the ratio.\n",
    "#SKIP THIS ONE\n",
    "components = pd.read_excel(xls_name, sheet_name=sheet_names[0], header=None)\n",
    "defects = pd.read_excel(xls_name, sheet_name=defect_sheet, header=None)\n",
    "attributes = pd.read_excel(xls_name, sheet_name='Attributes', header=0)\n",
    "comp = components[0].tolist()\n",
    "defe = defects[0].tolist()\n",
    "comp = [x.lower() for x in comp]\n",
    "defe = [x.lower() for x in defe]\n",
    "attr = np.array(attributes).flatten()\n",
    "attr = pd.DataFrame(attr).dropna()[0].tolist()\n",
    "#get number of unique components?\n",
    "#get number of unique defects?\n",
    "comp = np.unique(comp)\n",
    "comp = plusing(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'name \\'conj\\'is not defined'\n",
    "defe = [x[:-1] if x[-1]==' ' else x for x in defe]\n",
    "defe = np.unique(defe)\n",
    "defe = conj(defe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "defe = np.array(defe).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T R A I N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SKIP\n",
    "#For exlusive set trial\n",
    "def dupw(rnd):\n",
    "#     X_train_, X_test_, y_train_, y_test_ = train_test_split(sentences, ner_tags, test_size=.22222, random_state=rnd\n",
    "#     A_sen = np.hstack(X_train)\n",
    "#     A_tag = np.hstack(y_train)\n",
    "#     B_sen = np.hstack(X_test)\n",
    "#     B_tag = np.hstack(y_test)\n",
    "\n",
    "    A_sen = np.hstack([x[:,0] for x in train_set])\n",
    "    A_tag = np.hstack([x[:,1] for x in train_set])\n",
    "    B_sen = np.hstack([x[:,0] for x in test_set])\n",
    "    B_tag = np.hstack([x[:,1] for x in test_set])\n",
    "\n",
    "\n",
    "    A_idx = [i for i,x in enumerate(A_tag) if x in ['B-Attr', 'B-Component', 'B-Defect']]\n",
    "    B_idx = [i for i,x in enumerate(B_tag) if x in ['B-Attr', 'B-Component', 'B-Defect']]\n",
    "\n",
    "    len(np.unique(A_sen[A_idx])), len(np.unique(B_sen[B_idx]))\n",
    "\n",
    "    a = np.unique(A_sen[A_idx])\n",
    "    b = np.unique(B_sen[B_idx])\n",
    "\n",
    "    # Words in B but not in A\n",
    "    dup_words = [x for x in b if x not in a]\n",
    "    return dup_words,a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sent = s_related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sent = [x for x in dict_sent if len(x) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [word_tokenize(lemmatize_sentence(' '.join(s))) for s in sent_]\n",
    "#sentences dataset\n",
    "sentences = [np.array(x)[:,0].tolist() for x in dict_sent]\n",
    "#tags dataset\n",
    "ner_tags = [np.array(x)[:,1].tolist() for x in dict_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample max length : 81\n",
      "Sample avg length : 20.689912\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Checking overview of the dataset\n",
    "print('Sample max length : %d' % max(len(l) for l in sentences))\n",
    "print('Sample avg length : %f' % (sum(map(len, sentences))/len(sentences)))\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the tokens\n",
    "src_tokenizer = Tokenizer(oov_token='OOV') # define index 1 = OOV\n",
    "src_tokenizer.fit_on_texts(sentences)\n",
    "tar_tokenizer = Tokenizer(lower=False)\n",
    "tar_tokenizer.fit_on_texts(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 2250\n",
      "tag size : 8\n"
     ]
    }
   ],
   "source": [
    "#How many sentences/tokens used\n",
    "vocab_size = len(src_tokenizer.word_index) + 1\n",
    "# vocab_size = 4000\n",
    "tag_size = len(tar_tokenizer.word_index) + 1\n",
    "print('vocab size : {}'.format(vocab_size))\n",
    "print('tag size : {}'.format(tag_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'tokenize sentences and tags?'\n",
    "X_data = src_tokenizer.texts_to_sequences(sentences)\n",
    "y_data = tar_tokenizer.texts_to_sequences(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = src_tokenizer.word_index\n",
    "index_to_word = src_tokenizer.index_word\n",
    "ner_to_index = tar_tokenizer.word_index\n",
    "index_to_ner = tar_tokenizer.index_word\n",
    "index_to_ner[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'O', 2: 'B-Component', 3: 'B-Defect', 4: 'B-Attr', 5: 'I-Component', 6: 'I-Attr', 7: 'I-Defect', 0: 'PAD'}\n"
     ]
    }
   ],
   "source": [
    "print(index_to_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 70\n",
    "X_data = pad_sequences(X_data, padding='post', maxlen=max_len)\n",
    "y_data = pad_sequences(y_data, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,x_,y_ = train_test_split(sentences, ner_tags, test_size=.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1181"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2054, 352, 302, 21563)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O, I, B = 0, 0, 0\n",
    "B_component, B_defect, B_attr = 0, 0, 0\n",
    "\n",
    "for arr in x_:\n",
    "    for t in arr:\n",
    "        if t == 'O':\n",
    "            O += 1\n",
    "        elif t == 'B-Component':\n",
    "            B_component += 1\n",
    "        elif t == 'B-Defect':\n",
    "            B_defect += 1\n",
    "        elif t == 'B-Attr':\n",
    "            B_attr += 1\n",
    "        elif t[0] == 'I':\n",
    "            I += 1\n",
    "\n",
    "# number of components, defects, attributes tokens\n",
    "B_component, B_defect, B_attr, O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = to_categorical(y_test, num_classes=tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample size : (39944, 70)\n",
      "Train sample label size : (39944, 70, 7)\n",
      "Test sample size : (9986, 70)\n",
      "Test sample label size : (9986, 70, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How about validation?'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train sample size : {}'.format(X_train.shape))\n",
    "print('Train sample label size : {}'.format(y_train.shape))\n",
    "print('Test sample size : {}'.format(X_test.shape))\n",
    "print('Test sample label size : {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConcrete: \\nTrain sample size : (8032, 70)\\nTrain sample label size : (8032, 70, 8)\\nTest sample size : (2008, 70)\\nTest sample label size : (2008, 70, 8)\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Brick masonry:\n",
    "Train sample size : (80400, 70)\n",
    "Train sample label size : (80400, 70, 7)\n",
    "Test sample size : (20101, 70)\n",
    "Test sample label size : (20101, 70, 7)\n",
    "'''\n",
    "'''\n",
    "Concrete: \n",
    "Train sample size : (8032, 70)\n",
    "Train sample label size : (8032, 70, 8)\n",
    "Test sample size : (2008, 70)\n",
    "Test sample label size : (2008, 70, 8)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Pretrained Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "from keras.models import load_model\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1score(Callback):\n",
    "    def __init__(self, value = 0.0, use_char=True):\n",
    "        super(F1score, self).__init__()\n",
    "        self.value = value\n",
    "        self.use_char = use_char\n",
    "\n",
    "    def sequences_to_tags(self, sequences): # pred -> index_to_ner\n",
    "      result = []\n",
    "      for sequence in sequences: \n",
    "          tag = []\n",
    "          for pred in sequence: # get pred from sequence\n",
    "              pred_index = np.argmax(pred) # one hot encoding -> index\n",
    "              tag.append(index_to_ner[pred_index].replace(\"PAD\", \"O\")) # 'PAD' -> 'O'\n",
    "          result.append(tag)\n",
    "      return result\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "      if self.use_char:\n",
    "        X_test = self.validation_data[0]\n",
    "        X_char_test = self.validation_data[1]\n",
    "        y_test = self.validation_data[2]\n",
    "        y_predicted = self.model.predict([X_test, X_char_test])\n",
    "\n",
    "      else:\n",
    "        X_test = self.validation_data[0]\n",
    "        y_test = self.validation_data[1]\n",
    "        y_predicted = self.model.predict([X_test])\n",
    "\n",
    "      pred_tags = self.sequences_to_tags(y_predicted)\n",
    "      test_tags = self.sequences_to_tags(y_test)\n",
    "\n",
    "      score = f1_score(pred_tags, test_tags)\n",
    "      print(' - f1: {:04.2f}'.format(score * 100))\n",
    "      print(classification_report(test_tags, pred_tags))\n",
    "\n",
    "      if score > self.value:\n",
    "        print('f1_score improved from %f to %f, saving model to best_model.h5'%(self.value, score))\n",
    "        self.model.save('best_model.h5')\n",
    "        self.value = score\n",
    "      else:\n",
    "        print('f1_score did not improve from %f'%(self.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Google\n",
      "1: glove\n",
      "2: no pretrained\n",
      "3: pretrained with report\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# different pre-trained model\n",
    "pret_opt = ['Google', 'Glove', 'No pret', 'Pret report']\n",
    "pret_model = pret_opt[int(input('0: Google\\n1: glove\\n2: no pretrained\\n3: pretrained with report\\n'))]\n",
    "# pret_model = 'Pret report'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've not selected Google.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model (Google w2v)\n",
    "# Keep the Google model\n",
    "\n",
    "if pret_model == 'Google':\n",
    "    import gensim\n",
    "\n",
    "    word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "    print(word2vec_model.vectors.shape)\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    np.shape(embedding_matrix)\n",
    "\n",
    "    def get_vector(word):\n",
    "        if word in word2vec_model:\n",
    "            return word2vec_model[word]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    for word, i in src_tokenizer.word_index.items(): \n",
    "        temp = get_vector(word) \n",
    "        if temp is not None: \n",
    "            embedding_matrix[i] = temp\n",
    "else:\n",
    "    print(\"You've not selected Google.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model (Glove)\n",
    "\n",
    "if pret_model == 'Glove':\n",
    "    import numpy as np\n",
    "    embedding_dict = dict()\n",
    "    f = open('glove.6B/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "    \n",
    "    for line in f:\n",
    "        word_vector = line.split()\n",
    "        word = word_vector[0]\n",
    "        word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
    "        embedding_dict[word] = word_vector_arr\n",
    "    f.close()\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "\n",
    "    for word, i in src_tokenizer.word_index.items():\n",
    "        temp = embedding_dict.get(word)\n",
    "        if temp is not None:\n",
    "            embedding_matrix[i] = temp\n",
    "\n",
    "elif pret_model == 'Pret report':\n",
    "    from gensim.models import Word2Vec\n",
    "    import numpy as np\n",
    "\n",
    "    EMB_DIM = 100\n",
    "    w2v = Word2Vec.load(\"word2vec_iter10_DIM100.model\")\n",
    "    word_vectors = w2v.wv\n",
    "    embedding_matrix = word_vectors.vectors\n",
    "    print(\"You've selected facade report model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8393"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pret_model == 'Google':\n",
    "    embedding_layer = Embedding(vocab_length,\n",
    "                                300,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_len,\n",
    "                                trainable=True, mask_zero=True)\n",
    "elif pret_model == 'Glove':\n",
    "    embedding_layer = Embedding(vocab_length,\n",
    "                                100,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_len,\n",
    "                                trainable=True, mask_zero=True)\n",
    "elif pret_model == 'Pret report':\n",
    "    embedding_layer = Embedding(vocab_length,\n",
    "                                100,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_len,\n",
    "                                trainable=True, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bilab-ubuntu/.conda/envs/kpark/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/home/bilab-ubuntu/.conda/envs/kpark/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    }
   ],
   "source": [
    "# Bi-directional\n",
    "model = Sequential()\n",
    "if pret_model == 'No pret':\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len, mask_zero=True))\n",
    "else:\n",
    "    model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(50, activation='relu')))\n",
    "crf = CRF(tag_size)\n",
    "model.add(crf)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "# model.compile(loss=crf.loss_function, optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31955 samples, validate on 7989 samples\n",
      "Epoch 1/1\n",
      " 2944/31955 [=>............................] - ETA: 2:55 - loss: 6.4926 - crf_viterbi_accuracy: 0.8625"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-7c82d633a695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m'validation set: 0.2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF1score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_char\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/kpark/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.conda/envs/kpark/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kpark/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.conda/envs/kpark/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'validation set: 0.2'\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=1,  validation_split = 0.2, callbacks=[F1score(use_char=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['val_loss']) + 1)\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_crf_model = load_model('best_model.h5', custom_objects={'CRF':CRF,\n",
    "                                                  'crf_loss':crf_loss,\n",
    "                                                  'crf_viterbi_accuracy':crf_viterbi_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model performance evaluation\n",
    "\n",
    "f1score = F1score(use_char=False)\n",
    "\n",
    "'On testing set (0.2?)'\n",
    "y_predicted = bilstm_crf_model.predict([X_test])\n",
    "pred_tags = f1score.sequences_to_tags(y_predicted)\n",
    "test_tags = f1score.sequences_to_tags(y_test)\n",
    "print(classification_report(test_tags, pred_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check manual sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_sen = np.hstack(X_train)\n",
    "A_tag = np.hstack(y_train)\n",
    "B_sen = np.hstack(X_test)\n",
    "B_tag = np.hstack(y_test)\n",
    "\n",
    "np.unique(A_tag)\n",
    "\n",
    "A_idx = [i for i,x in enumerate(A_tag) if x in ['B-Attr', 'B-Component', 'B-Defect']]\n",
    "B_idx = [i for i,x in enumerate(B_tag) if x in ['B-Attr', 'B-Component', 'B-Defect']]\n",
    "\n",
    "len(np.unique(A_sen[A_idx])), len(np.unique(B_sen[B_idx]))\n",
    "\n",
    "a = np.unique(A_sen[A_idx])\n",
    "b = np.unique(B_sen[B_idx])\n",
    "\n",
    "yo = [x for x in b if x not in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence= X_test_[5208]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_encoded=[]\n",
    "for w in new_sentence:\n",
    "    try:\n",
    "      new_encoded.append(word_to_index.get(w,1))\n",
    "    except KeyError:\n",
    "      new_encoded.append(word_to_index['OOV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_padded = pad_sequences([new_encoded], padding=\"post\", value=0, maxlen=max_len)\n",
    "\n",
    "p = bilstm_crf_model.predict(np.array([new_padded[0]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:15}||{}\".format(\"word\", \"pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, pred in zip(new_sentence, p[0]):\n",
    "    print(\"{:15}: {:5}\".format(w, index_to_ner[pred]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpark",
   "language": "python",
   "name": "kpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
